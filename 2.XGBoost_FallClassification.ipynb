{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a regression tree based ensemble learning technique. It has several noticeable improvements over classical gradient boosting. The major difference between gradient boosting and XGBoost is the former can be viewed in term of gradient descent while XGBoost belongs to Newton Boosting, using the second order derivative of the loss function to make a better approximation about the direction of maximum decrease. This allows the model to converge at a faster rate.  \n",
    "\n",
    "In this work, XGBoost algorithm was coded from scratch using the exact greedy search approach for the tree method. This approach was selected as it could provide the most accurate result.  \n",
    "\n",
    "For an optimal modal, tuning the hyperparameter is required.   \n",
    "To control overfitting :\n",
    "1. Directly control model complexity:\n",
    "    max_depth, min_child_weight and gamma.\n",
    "2. To add randomness to make training robust to noise:\n",
    "    -subsample and colsample_bytree (random features used to train each tree.)\n",
    "    -reduce learning rate and n_estimators (boosting rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:\\XGBoost\\dataset\\sisfall_processed_100Hz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean_Ax</th>\n",
       "      <th>Std_Ax</th>\n",
       "      <th>Var_Ax</th>\n",
       "      <th>Range_Ax</th>\n",
       "      <th>Mean_Ay</th>\n",
       "      <th>Std_Ay</th>\n",
       "      <th>Var_Ay</th>\n",
       "      <th>Range_Ay</th>\n",
       "      <th>Mean_Az</th>\n",
       "      <th>Std_Az</th>\n",
       "      <th>...</th>\n",
       "      <th>Range_SVM</th>\n",
       "      <th>Mean_SVM_Horizontal</th>\n",
       "      <th>Std_SVM_Horizontal</th>\n",
       "      <th>Var_SVM_Horizontal</th>\n",
       "      <th>Range_SVM_Horizontal</th>\n",
       "      <th>Mean_Angle_z_xy</th>\n",
       "      <th>Std_Angle_z_xy</th>\n",
       "      <th>Var_Angle_z_xy</th>\n",
       "      <th>Range_Angle_z_xy</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024154</td>\n",
       "      <td>0.017176</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>-0.997331</td>\n",
       "      <td>0.020355</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.112057</td>\n",
       "      <td>0.069145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145789</td>\n",
       "      <td>0.132886</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>1.458043</td>\n",
       "      <td>0.067067</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>0.400908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009993</td>\n",
       "      <td>0.015422</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>-0.974901</td>\n",
       "      <td>0.052550</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.305284</td>\n",
       "      <td>0.185734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152861</td>\n",
       "      <td>0.305947</td>\n",
       "      <td>0.185550</td>\n",
       "      <td>0.034429</td>\n",
       "      <td>0.531262</td>\n",
       "      <td>1.871868</td>\n",
       "      <td>0.183874</td>\n",
       "      <td>0.033810</td>\n",
       "      <td>0.520666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008552</td>\n",
       "      <td>0.010665</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>-1.019453</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>-0.061072</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075036</td>\n",
       "      <td>0.062571</td>\n",
       "      <td>0.019394</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.116146</td>\n",
       "      <td>1.630598</td>\n",
       "      <td>0.018832</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.114490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028919</td>\n",
       "      <td>0.022021</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>-0.981068</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>-0.163958</td>\n",
       "      <td>0.299944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256702</td>\n",
       "      <td>0.274750</td>\n",
       "      <td>0.206211</td>\n",
       "      <td>0.042523</td>\n",
       "      <td>0.578840</td>\n",
       "      <td>1.727167</td>\n",
       "      <td>0.291886</td>\n",
       "      <td>0.085198</td>\n",
       "      <td>0.741900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011057</td>\n",
       "      <td>0.013112</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>-1.012497</td>\n",
       "      <td>0.017493</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.045383</td>\n",
       "      <td>0.055758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096304</td>\n",
       "      <td>0.050046</td>\n",
       "      <td>0.054379</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.273577</td>\n",
       "      <td>1.616008</td>\n",
       "      <td>0.056829</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.295768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean_Ax    Std_Ax    Var_Ax  Range_Ax   Mean_Ay    Std_Ay    Var_Ay  \\\n",
       "0  0.024154  0.017176  0.000295  0.121094 -0.997331  0.020355  0.000414   \n",
       "1  0.009993  0.015422  0.000238  0.105469 -0.974901  0.052550  0.002762   \n",
       "2  0.008552  0.010665  0.000114  0.078125 -1.019453  0.008987  0.000081   \n",
       "3  0.028919  0.022021  0.000485  0.136719 -0.981068  0.052425  0.002748   \n",
       "4  0.011057  0.013112  0.000172  0.089844 -1.012497  0.017493  0.000306   \n",
       "\n",
       "   Range_Ay   Mean_Az    Std_Az  ...  Range_SVM  Mean_SVM_Horizontal  \\\n",
       "0  0.156250  0.112057  0.069145  ...   0.145789             0.132886   \n",
       "1  0.183594 -0.305284  0.185734  ...   0.152861             0.305947   \n",
       "2  0.074219 -0.061072  0.019361  ...   0.075036             0.062571   \n",
       "3  0.281250 -0.163958  0.299944  ...   0.256702             0.274750   \n",
       "4  0.132812 -0.045383  0.055758  ...   0.096304             0.050046   \n",
       "\n",
       "   Std_SVM_Horizontal  Var_SVM_Horizontal  Range_SVM_Horizontal  \\\n",
       "0            0.023295            0.000543              0.230243   \n",
       "1            0.185550            0.034429              0.531262   \n",
       "2            0.019394            0.000376              0.116146   \n",
       "3            0.206211            0.042523              0.578840   \n",
       "4            0.054379            0.002957              0.273577   \n",
       "\n",
       "   Mean_Angle_z_xy  Std_Angle_z_xy  Var_Angle_z_xy  Range_Angle_z_xy  label  \n",
       "0         1.458043        0.067067        0.004498          0.400908      0  \n",
       "1         1.871868        0.183874        0.033810          0.520666      0  \n",
       "2         1.630598        0.018832        0.000355          0.114490      0  \n",
       "3         1.727167        0.291886        0.085198          0.741900      0  \n",
       "4         1.616008        0.056829        0.003230          0.295768      0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mean_Ax                 0\n",
       "Std_Ax                  0\n",
       "Var_Ax                  0\n",
       "Range_Ax                0\n",
       "Mean_Ay                 0\n",
       "Std_Ay                  0\n",
       "Var_Ay                  0\n",
       "Range_Ay                0\n",
       "Mean_Az                 0\n",
       "Std_Az                  0\n",
       "Var_Az                  0\n",
       "Range_Az                0\n",
       "Mean_SVM                0\n",
       "Std_SVM                 0\n",
       "Var_SVM                 0\n",
       "Range_SVM               0\n",
       "Mean_SVM_Horizontal     0\n",
       "Std_SVM_Horizontal      0\n",
       "Var_SVM_Horizontal      0\n",
       "Range_SVM_Horizontal    0\n",
       "Mean_Angle_z_xy         0\n",
       "Std_Angle_z_xy          0\n",
       "Var_Angle_z_xy          0\n",
       "Range_Angle_z_xy        0\n",
       "label                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() #to ensure no missing value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2702\n",
       "1    1798\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the classes, functions and parameters used for XGBoost Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a regression tree, class node was defined. \n",
    "Trees are recursive data structures, so the node class represents a decision point in the model. That decision point will divide the data into two sets (left and right).\n",
    "\n",
    "The principle of the regression tree is based on https://curiousily.com/posts/build-a-decision-tree-from-scratch-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, X, gradient, hessian, idxs, colsample_features, min_leaf = 5, min_child_weight = 1 ,depth = 6, lambda_ = 1, gamma = 1):\n",
    "      \n",
    "        self.X = X #Training data in dataframe\n",
    "        self.gradient = gradient #negative first derivative of loss function\n",
    "        self.hessian = hessian #second derivative of loss function\n",
    "        self.idxs = idxs #stores indexes of subset of the data that the particular node is working with\n",
    "        self.depth = depth #maximum depth of a tree\n",
    "        self.min_leaf = min_leaf #min number of samples for a node to be considered a node\n",
    "        self.lambda_ = lambda_ #regularization term\n",
    "        self.gamma  = gamma #prevent overfitting \n",
    "        self.min_child_weight = min_child_weight #controls model complexity (hessian)\n",
    "        self.row_count = len(idxs) #no of samples of each node is working with\n",
    "        self.colsample_features = colsample_features #every tree is trained with different features\n",
    "        #self.colsample_by_tree = colsample_by_tree #random features used to train each tree.\n",
    "        \n",
    "        self.val = -np.sum(gradient[self.idxs])/(np.sum(hessian[self.idxs]) + self.lambda_) #output value for the leaf node         \n",
    "        self.score = float(\"-inf\") #internal gain score is initially set to infinity -> only leaf nodes will have infinity score.\n",
    "        self.find_varsplit()\n",
    "        \n",
    "        \n",
    "    def find_varsplit(self):\n",
    "\n",
    "        for c in self.colsample_features: #scans through each colume (features)\n",
    "            self.find_better_split(c) #find better feature to split on using the exact greed search approach\n",
    "            \n",
    "        if self.is_leaf: #if not such feature [at leaf node], do nothing (no splits)\n",
    "            return #return None\n",
    "        \n",
    "        X = self.X.values[self.idxs , self.var_idx] #split based on the \"best\" feature\n",
    "        \n",
    "        #create data for left and right nodes (return the indices)\n",
    "        lhs = np.nonzero(X <= self.split)[0]\n",
    "        rhs = np.nonzero(X > self.split)[0]\n",
    "        \n",
    "        #create left and right node using the subset of the data - building recursively\n",
    "        #Depth is only parameter to change as a new layer is added to tree structure.\n",
    "        self.lhs = Node(X = self.X, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[lhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, colsample_features = self.colsample_features)\n",
    "        self.rhs = Node(X = self.X, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[rhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, colsample_features = self.colsample_features)\n",
    "        \n",
    "    def find_better_split(self, var_idx):\n",
    "        \n",
    "        X = self.X.values[self.idxs, var_idx] #return a 1d list of all the sample value for a given feature\n",
    "        \n",
    "        #trying to split on each sample (datapoint) and let the best split wins with the highest score\n",
    "        for r in range(self.row_count):\n",
    "            lhs = X <= X[r] #return True/False in list\n",
    "            rhs = X > X[r]\n",
    "            \n",
    "            lhs_indices = np.nonzero(X <= X[r])[0] # index lhs of split\n",
    "            rhs_indices = np.nonzero(X > X[r])[0] # index rhs of split\n",
    "            \n",
    "            #continue until the stopping criteria is met\n",
    "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf \n",
    "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
    "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
    "\n",
    "            #In XGBoost, internal gain is used to find the optimal split value uses both the gradient and hessian\n",
    "            curr_score = self.gain(lhs, rhs) \n",
    "            \n",
    "            \n",
    "            if curr_score > self.score: #globally updates the better score and split point \n",
    "                self.var_idx = var_idx #best feature - index of the split variable\n",
    "                self.score = curr_score #gain score\n",
    "                self.split = X[r]  #best \"threshold\" - value of the split\n",
    "\n",
    "    def gain(self, lhs, rhs): #calculate the gain at a particular split point\n",
    "        \n",
    "        gradient = self.gradient[self.idxs] #gradient value for every sample \n",
    "        hessian  = self.hessian[self.idxs] #hessian value for every sample\n",
    "        \n",
    "        lhs_gradient = gradient[lhs].sum()\n",
    "        lhs_hessian  = hessian[lhs].sum()\n",
    "        \n",
    "        rhs_gradient = gradient[rhs].sum()\n",
    "        rhs_hessian  = hessian[rhs].sum()\n",
    "        \n",
    "        gain = 0.5 *((lhs_gradient**2/(lhs_hessian + self.lambda_)) + (rhs_gradient**2/(rhs_hessian + self.lambda_)) - ((lhs_gradient + rhs_gradient)**2/(lhs_hessian + rhs_hessian + self.lambda_))) - self.gamma\n",
    "        return(gain)\n",
    "                \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        #score is initialize as infinity and only leaf node has infinity score (gain)\n",
    "        return self.score == float('-inf') or self.depth <= 0                 \n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_row(x) for x in X]) #the prediction is stored in array. \n",
    "    \n",
    "    def predict_row(self, x):\n",
    "    #check if need to go left or right node based on the split value and feature found\n",
    "        if self.is_leaf: #if leaf node, compute the output value\n",
    "            return(self.val)\n",
    "\n",
    "        node = self.lhs if x[self.var_idx] <= self.split else self.rhs\n",
    "        return node.predict_row(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the regression tree \n",
    "class XGBoostTree:\n",
    "    def fit(self,X,gradient,hessian,colsample_features,min_leaf=5,min_child_weight=1,depth=6,lambda_=1,gamma=1):\n",
    "        self.XGBoosttree = Node(X,gradient,hessian,np.array(np.arange(len(X))),colsample_features,min_leaf,min_child_weight,depth,lambda_, gamma)\n",
    "        return self\n",
    "       \n",
    "    def predict(self, X):\n",
    "        return self.XGBoosttree.predict(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Classification (XGBoost Tree in loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): #function to convert from log odds to probability format\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#gradient is the negative derivative of log loss\n",
    "#prediciton in probability, y_label is the true label of the sample\n",
    "def cal_gradient(prediction,y_label):\n",
    "    p = sigmoid(prediction)\n",
    "    return (p-y_label)\n",
    "    \n",
    "#hessian is the second derivative of the log loss\n",
    "def cal_hessian(prediction):\n",
    "    p = sigmoid(prediction)\n",
    "    return (p*(1-p))\n",
    "\n",
    "#XGBoost will subsample the ratio of the training instances to train a single tree\n",
    "#randomly select the mentioned ratio of training data prior to grow a tree \n",
    "#to reduce the variance and the dependence on certain values in the training sample\n",
    "def subsample(X_train, y_train, subsample_ratio):\n",
    "    n_samples = len(X_train)\n",
    "    n_samples = np.arange(n_samples)\n",
    "    #idxs = random.sample(n_samples, np.round(subsample_ratio*n_samples)) #without replacement\n",
    "    idxs = np.random.choice(n_samples, round(subsample_ratio*len(n_samples)), replace=False)\n",
    "    return X_train.iloc[idxs], y_train.iloc[idxs]\n",
    "\n",
    "#reduce the variance of the models by training on different parts of the data.\n",
    "#each tree is trained with random features \n",
    "def colsample_bytree(X_train, colsample_ratio):\n",
    "    n_features = X.shape[1]\n",
    "    feature_idxs = np.random.choice(n_features, round(colsample_ratio*n_features), replace=True)\n",
    "    return feature_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute TP, TN, FN, FP\n",
    "'''\n",
    "True_positive: Actual = 1, Predicted = 1\n",
    "False_positive: Acutal = 0, Predicted = 1\n",
    "True_negative: Acutal = 0, Predicted = 0\n",
    "False_negative: Acutal = 1, Predicted = 0\n",
    "'''\n",
    "def compute_TP_TN_FP_FN(y_actual, y_predict):\n",
    "    tp = sum((y_actual == 1) & (y_predict == 1))\n",
    "    fp = sum((y_actual == 0) & (y_predict == 1))\n",
    "    tn = sum((y_actual == 0) & (y_predict == 0))\n",
    "    fn = sum((y_actual == 1) & (y_predict == 0))\n",
    "    \n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def accuracy(TP, TN, FP, FN):\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    return acc\n",
    "\n",
    "def sensitivity (TP, FN): #recall\n",
    "    sensi = TP / (TP + FN)\n",
    "    return sensi\n",
    "\n",
    "def specificity (TN, FP):\n",
    "    speci = TN / (TN + FP)\n",
    "    return speci\n",
    "\n",
    "def precision(TP, FP):\n",
    "    preci = TP / (TP + FP)\n",
    "    return preci\n",
    "\n",
    "def F1_Score(precision, sensitivity): \n",
    "#maximum score of 1 (perfect precision and recall) and a minimum of 0 \n",
    "    F1 = (2 * precision * sensitivity) / (precision + sensitivity)\n",
    "    return F1\n",
    "\n",
    "def compute_FPR(sensitivity): #False positive rate \n",
    "    FPR = 1 - sensitivity\n",
    "    return FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "learning_rate = 0.3\n",
    "lambda_ = 1\n",
    "gamma = 1\n",
    "depth = 10\n",
    "min_child_weight = 1\n",
    "subsample_ratio = 1\n",
    "colsample_ratio = 0.8\n",
    "boosting_rounds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Fold cross validation to train the model - prevent overfitting. Each fold: 450 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n",
      "Ori_n_estimators.shape (0,)\n",
      "Size of n_estimators (10,)\n",
      "pred (450,)\n"
     ]
    }
   ],
   "source": [
    "k = StratifiedKFold(n_splits=10, shuffle = True)\n",
    " \n",
    "TP_ = []\n",
    "FP_ = []\n",
    "TN_ = []\n",
    "FN_ = []\n",
    "acc_ = []\n",
    "sensi_ = []\n",
    "speci_ = []\n",
    "preci_ = []\n",
    "F1_ = []\n",
    "FPR_ = []\n",
    "\n",
    "for train_index, test_index in k.split(X,y):\n",
    "    #train = df.iloc[train_index]\n",
    "    #test = df.iloc[test_index] \n",
    "    #split the dataset into training and test set\n",
    "    #X_train, y_train = train.iloc[:,:-1], train.iloc[:,-1]\n",
    "    #X_test, y_test = test.iloc[:,:-1], test.iloc[:,-1]\n",
    "    \n",
    "    n_estimators = []\n",
    "    #print(\"Ori_n_estimators.shape\", np.shape(n_estimators))\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "               \n",
    "    #train the classifier with X_train\n",
    "    base_prediction = np.full((round(subsample_ratio*X_train.shape[0]),1),1).flatten().astype('float64') \n",
    "    \n",
    "    #for each regression tree    \n",
    "    #boosting_rounds refer to the n_estimators in XGBoost algorithm. \n",
    "    #numbers of the regression tree to be built\n",
    "    for i in range (boosting_rounds):   \n",
    "        \n",
    "        #randomly select samples to train the model \n",
    "        random_X, random_y = subsample(X_train, y_train, subsample_ratio)\n",
    "            \n",
    "        #randomly select features to train the model\n",
    "        colsample_features = colsample_bytree(X_train, colsample_ratio)\n",
    "\n",
    "        #use the initial prediction to calculate gradient and hessian\n",
    "        gradient = cal_gradient(base_prediction, random_y.values) #return 1d array\n",
    "        hessian = cal_hessian(base_prediction) #return 1d array\n",
    "            \n",
    "        #build the tree\n",
    "        tree = XGBoostTree()\n",
    "        boosting_tree = tree.fit(random_X,gradient,hessian,colsample_features,min_leaf=5,min_child_weight=1,depth=8,lambda_=1,gamma=1)\n",
    "            \n",
    "        #predict the boosting tree and update the base prediction \n",
    "        base_prediction += learning_rate * boosting_tree.predict(random_X)\n",
    "        n_estimators.append(boosting_tree) #save the regreesion tree to the list        \n",
    "        \n",
    "    #Prediction for X_test\n",
    "    \n",
    "    #create an empty array to store the latest prediction for X_test\n",
    "    prediction = np.zeros(X_test.shape[0]) #initial prediction of 0 for all the testing data\n",
    "\n",
    "    #initial prediction for testing data, which is 1\n",
    "    initial_pred = np.full((X_test.shape[0],1),1).flatten().astype('float64')\n",
    "    \n",
    "    #update the prediction for every new created regression tree \n",
    "    #for every tree in all the boosting trees created\n",
    "    for estimator in n_estimators:\n",
    "        #new_prediction is in log_odds format for X_test\n",
    "        prediction += learning_rate * estimator.predict(X_test) #predict the boosting tree  \n",
    "        \n",
    "    #final prediction in log odds format\n",
    "    new_prediction = initial_pred + prediction\n",
    "    #convert from log_odds into probability\n",
    "    new_prediction_prob = sigmoid(new_prediction)\n",
    "    \n",
    "    #set a threshold\n",
    "    #if the new_prediction > \"value\", classify into 1 - high risk and vice versa\n",
    "    #prediction for X_test\n",
    "    y_pred = np.where(new_prediction_prob > np.mean(new_prediction_prob), 1, 0)\n",
    "    print(\"pred\", np.shape(y_pred))\n",
    "\n",
    "                \n",
    "    #evaluation for test set btw y_test and y_pred\n",
    "    TP, FP, TN, FN = compute_TP_TN_FP_FN(y_test, y_pred)\n",
    "    TP_.append(TP), FP_.append(FP), TN_.append(TN), FN_.append(FN)\n",
    "    acc = accuracy(TP, TN, FP, FN)\n",
    "    acc_.append(acc)\n",
    "    sensi = sensitivity (TP, FN)\n",
    "    sensi_.append(sensi)\n",
    "    speci = specificity (TN, FP)\n",
    "    speci_.append(speci)\n",
    "    preci = precision(TP, FP)\n",
    "    preci_.append(preci)\n",
    "    F1 = F1_Score(preci, sensi)\n",
    "    F1_.append(F1)\n",
    "    FPR = compute_FPR(sensi)\n",
    "    FPR_.append(FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  [179, 175, 176, 173, 172, 178, 178, 174, 177, 175]\n",
      "FP:  [9, 2, 3, 5, 6, 11, 9, 4, 7, 7]\n",
      "TN:  [262, 269, 267, 265, 264, 259, 261, 266, 263, 263]\n",
      "FN:  [0, 4, 4, 7, 8, 2, 2, 6, 3, 5]\n",
      "Accuracy:  [0.98, 0.9866666666666667, 0.9844444444444445, 0.9733333333333334, 0.9688888888888889, 0.9711111111111111, 0.9755555555555555, 0.9777777777777777, 0.9777777777777777, 0.9733333333333334]\n",
      "Sensitivity:  [1.0, 0.9776536312849162, 0.9777777777777777, 0.9611111111111111, 0.9555555555555556, 0.9888888888888889, 0.9888888888888889, 0.9666666666666667, 0.9833333333333333, 0.9722222222222222]\n",
      "Specificity:  [0.966789667896679, 0.992619926199262, 0.9888888888888889, 0.9814814814814815, 0.9777777777777777, 0.9592592592592593, 0.9666666666666667, 0.9851851851851852, 0.9740740740740741, 0.9740740740740741]\n",
      "Precision:  [0.9521276595744681, 0.9887005649717514, 0.9832402234636871, 0.9719101123595506, 0.9662921348314607, 0.9417989417989417, 0.9518716577540107, 0.9775280898876404, 0.9619565217391305, 0.9615384615384616]\n",
      "F1_Score:  [0.9754768392370572, 0.9831460674157303, 0.9805013927576602, 0.9664804469273743, 0.9608938547486034, 0.9647696476964769, 0.9700272479564033, 0.9720670391061451, 0.9725274725274725, 0.9668508287292817]\n",
      "FPR:  [0.0, 0.022346368715083775, 0.022222222222222254, 0.03888888888888886, 0.0444444444444444, 0.011111111111111072, 0.011111111111111072, 0.033333333333333326, 0.01666666666666672, 0.02777777777777779]\n"
     ]
    }
   ],
   "source": [
    "print(\"TP: \", TP_)\n",
    "print(\"FP: \", FP_)\n",
    "print(\"TN: \", TN_)\n",
    "print(\"FN: \", FN_)\n",
    "print(\"Accuracy: \", acc_)\n",
    "print(\"Sensitivity: \", sensi_)\n",
    "print(\"Specificity: \", speci_)\n",
    "print(\"Precision: \", preci_)\n",
    "print(\"F1_Score: \", F1_)\n",
    "print(\"FPR: \", FPR_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean value for the 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  176\n",
      "FP:  6\n",
      "TN:  264\n",
      "FN:  4\n"
     ]
    }
   ],
   "source": [
    "TP_mean= round(np.mean(TP_))\n",
    "print(\"TP: \", TP_mean)\n",
    "FP_mean = round(np.mean(FP_))\n",
    "print(\"FP: \", FP_mean)\n",
    "TN_mean= round(np.mean(TN_))\n",
    "print(\"TN: \", TN_mean)\n",
    "FN_mean= round(np.mean(FN_))\n",
    "print(\"FN: \", FN_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.98 +- 0.01\n",
      "Sensitivity:  0.98 +- 0.01\n",
      "Specificity:  0.98 +- 0.98\n",
      "Precision:  0.97 +- 0.01\n",
      "F1:  0.97 +- 0.01\n",
      "FPR:  0.02 +- 0.01\n"
     ]
    }
   ],
   "source": [
    "Acc_mean = round(np.mean(acc_),2)\n",
    "Acc_std = round(np.std(acc_),2)\n",
    "print(\"Accuracy: \", Acc_mean, \"+-\", Acc_std)\n",
    "\n",
    "Sensi_mean = round(np.mean(sensi_),2)\n",
    "Sensi_std= round(np.std(sensi_),2)\n",
    "print(\"Sensitivity: \", Sensi_mean, \"+-\", Sensi_std)\n",
    "\n",
    "Speci_mean = round(np.mean(speci_),2)\n",
    "Speci_std = round(np.mean(speci_),2)\n",
    "print(\"Specificity: \", Speci_mean, \"+-\", Speci_std)\n",
    "\n",
    "Preci_mean = round(np.mean(preci_),2)\n",
    "Preci_std = round(np.std(preci_),2)\n",
    "print(\"Precision: \", Preci_mean, \"+-\", Preci_std)\n",
    "\n",
    "F1_mean = round(np.mean(F1_),2)\n",
    "F1_std = round(np.std(F1_),2)\n",
    "print(\"F1: \", F1_mean, \"+-\", F1_std)\n",
    "\n",
    "FPR_mean = round(np.mean(FPR_),2)\n",
    "FPR_std = round(np.std(FPR_),2)\n",
    "print(\"FPR: \", FPR_mean, \"+-\", FPR_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1234,stratify=y)\n",
    "\n",
    "TP_ = []\n",
    "FP_ = []\n",
    "TN_ = []\n",
    "FN_ = []\n",
    "acc_ = []\n",
    "sensi_ = []\n",
    "speci_ = []\n",
    "preci_ = []\n",
    "F1_ = []\n",
    "FPR_ = []\n",
    "\n",
    "for boosting_round in [5,10,15]:   #changing the parameter\n",
    "    n_estimators = []\n",
    "    print(\"Ori_n_estimators.shape\", np.shape(n_estimators))\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "               \n",
    "    #train the classifier with X_train\n",
    "    base_prediction = np.full((round(subsample_ratio*X_train.shape[0]),1),1).flatten().astype('float64') \n",
    "    \n",
    "    #for each regression tree    \n",
    "    #boosting_rounds refer to the n_estimators in XGBoost algorithm. \n",
    "    #numbers of the regression tree to be built\n",
    "    for i in range (boosting_rounds):   \n",
    "        \n",
    "        #randomly select samples to train the model \n",
    "        random_X, random_y = subsample(X_train, y_train, subsample_ratio)\n",
    "            \n",
    "        #randomly select features to train the model\n",
    "        colsample_features = colsample_bytree(X_train, colsample_ratio)\n",
    "\n",
    "        #use the initial prediction to calculate gradient and hessian\n",
    "        gradient = cal_gradient(base_prediction, random_y.values) #return 1d array\n",
    "        hessian = cal_hessian(base_prediction) #return 1d array\n",
    "            \n",
    "        #build the tree\n",
    "        tree = XGBoostTree()\n",
    "        boosting_tree = tree.fit(random_X,gradient,hessian,colsample_features,min_leaf=5,min_child_weight=1,depth=8,lambda_=1,gamma=1)\n",
    "            \n",
    "        #predict the boosting tree and update the base prediction \n",
    "        base_prediction += learning_rate * boosting_tree.predict(random_X)\n",
    "        n_estimators.append(boosting_tree) #save the regreesion tree to the list        \n",
    "        \n",
    "    #prediction for X_test\n",
    "    print(\"Size of n_estimators\", np.shape(n_estimators))\n",
    "    #create an empty array to store the latest prediction for X_test\n",
    "    prediction = np.zeros(X_test.shape[0]) #initial prediction of 0 for all the testing data\n",
    "\n",
    "    #initial prediction for testing data, which is 1\n",
    "    initial_pred = np.full((X_test.shape[0],1),1).flatten().astype('float64')\n",
    "    \n",
    "    #update the prediction for every new created regression tree \n",
    "    #for every tree in all the boosting trees created\n",
    "    for estimator in n_estimators:\n",
    "        #new_prediction is in log_odds format for X_test\n",
    "        prediction += learning_rate * estimator.predict(X_test) #predict the boosting tree  \n",
    "        \n",
    "    #final prediction in log odds format\n",
    "    new_prediction = initial_pred + prediction\n",
    "    #convert from log_odds into probability\n",
    "    new_prediction_prob = sigmoid(new_prediction)\n",
    "    \n",
    "    #set a threshold\n",
    "    #if the new_prediction > \"value\", classify into 1 - high risk and vice versa\n",
    "    #prediction for X_test\n",
    "    y_pred = np.where(new_prediction_prob > np.mean(new_prediction_prob), 1, 0)\n",
    "    print(\"pred\", np.shape(y_pred))\n",
    "\n",
    "                \n",
    "    #evaluation for test set btw y_test and y_pred\n",
    "    TP, FP, TN, FN = compute_TP_TN_FP_FN(y_test, y_pred)\n",
    "    TP_.append(TP), FP_.append(FP), TN_.append(TN), FN_.append(FN)\n",
    "    acc = accuracy(TP, TN, FP, FN)\n",
    "    acc_.append(acc)\n",
    "    sensi = sensitivity (TP, FN)\n",
    "    sensi_.append(sensi)\n",
    "    speci = specificity (TN, FP)\n",
    "    speci_.append(speci)\n",
    "    preci = precision(TP, FP)\n",
    "    preci_.append(preci)\n",
    "    F1 = F1_Score(preci, sensi)\n",
    "    F1_.append(F1)\n",
    "    FPR = compute_FPR(sensi)\n",
    "    FPR_.append(FPR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
